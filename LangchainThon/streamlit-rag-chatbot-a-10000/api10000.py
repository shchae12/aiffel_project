# api.py
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import requests
import streamlit as st
import xml.etree.ElementTree as ET
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import time
import re

# 2. ê³µê³µë°ì´í„° APIì—ì„œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ìˆ˜ì§‘ (XML íŒŒì‹±)
def fetch_health_product_data(start_page=1, num_pages=1, delay=0.2):
    url = "http://apis.data.go.kr/1471000/HtfsInfoService03/getHtfsItem01"
    service_key = st.secrets["PUBLIC_API_KEY"]

    all_items = []
    for page in range(start_page, start_page + num_pages):
        params = {
            'serviceKey': service_key,
            'pageNo': page,
            'numOfRows': 100,
            'type': 'xml'
        }
        try:
            response = requests.get(url, params=params, verify=False)
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
            if not items:
                print(f"ğŸš« {page}í˜ì´ì§€ ë°ì´í„° ì—†ìŒ, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            for item in items:
                row = {child.tag: child.text for child in item}
                all_items.append(row)
            
            time.sleep(delay)
        except Exception as e:
            print(f"âš ï¸ {page}í˜ì´ì§€ ì—ëŸ¬: {e}")
            continue
    print(f"âœ… {start_page}~{start_page + num_pages - 1} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_items)}ê°œ")
    return all_items

# 3. ê° item â†’ LangChain Documentë¡œ ë³€í™˜
def item_to_document(item):
    text = f"""ì œí’ˆëª…: {item.get('PRDUCT', '')}
ì—…ì²´ëª…: {item.get('ENTRPS', '')}
ê¸°ëŠ¥ì„±: {item.get('MAIN_FNCTN', '')}
ì£¼ì˜ì‚¬í•­: {item.get('INTAKE_HINT1', '')}
ì„­ì·¨ë°©ë²•: {item.get('SRV_USE', '')}
í‘œì¤€ì„±ë¶„: {item.get('BASE_STANDARD', '')}
"""
    return Document(page_content=text, metadata={"product": item.get("PRDUCT", "")})

# 4. ë²¡í„° ì„ë² ë”© ë° FAISS ë²¡í„° DB ì´ˆê¸°í™”
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
embedding = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=os.environ["OPENAI_API_KEY"]
)

# all_documents = []
db = None  # ìµœì´ˆ FAISS ì¸ìŠ¤í„´ìŠ¤ëŠ” ì²« 500ê°œë¡œ ìƒì„±

# ì•ˆì „í•˜ê²Œ ë¬¸ì„œ 100ê°œì”© ë‚˜ëˆ ì„œ FAISSì— ì¶”ê°€ - ì„ë² ë”© í† í° ì´ˆê³¼ ì—ëŸ¬ ë°œìƒ
def add_documents_in_chunks(db, documents, chunk_size=100):
    for i in range(0, len(documents), chunk_size):
        chunk = documents[i:i + chunk_size]
        db.add_documents(chunk)

# 5. ì „ì²´ 10000ê°œ ë°ì´í„°ë¥¼ 500ê°œ ë‹¨ìœ„ë¡œ ìˆ˜ì§‘, ì„ë² ë”©
TOTAL_PAGES = 100
BATCH_SIZE = 500  # â†’ 500ê°œì”© ì„ë² ë”©
PAGES_PER_BATCH = BATCH_SIZE // 100  # 5í˜ì´ì§€ì”© = 500ê°œ

for i in range(0, TOTAL_PAGES, PAGES_PER_BATCH):
    start_page = i + 1
    print(f"\nğŸ“¦ Batch {i // PAGES_PER_BATCH + 1} (í˜ì´ì§€ {start_page}~{start_page + PAGES_PER_BATCH - 1}) ìˆ˜ì§‘ ì¤‘...")
    
    items = fetch_health_product_data(start_page=start_page, num_pages=PAGES_PER_BATCH)
    documents = [item_to_document(item) for item in items]

    if not documents:
        print("âŒ ë¬¸ì„œ ì—†ìŒ â†’ ë£¨í”„ ì¢…ë£Œ")
        break

    if db is None:
        db = FAISS.from_documents(documents, embedding)
        print(f"ğŸ§  ì´ˆê¸° ë²¡í„° ì €ì¥ì†Œ ìƒì„±! (ë¬¸ì„œ ìˆ˜: {len(documents)})")
    else:
        add_documents_in_chunks(db, documents, chunk_size=100)
        print(f"â• ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ ì™„ë£Œ! ëˆ„ì  ë¬¸ì„œ ìˆ˜: {len(db.docstore._dict)}")

db.save_local("./faiss_index/db")
print(f"\nâœ… ì „ì²´ ë²¡í„° DB ìƒì„± ì™„ë£Œ! ì´ ë¬¸ì„œ ìˆ˜: {len(db.docstore._dict)}ê°œ")

# ì¿¼ë¦¬ -> ìœ ì‚¬ë„ ê²€ìƒ‰ -> ê²°ê³¼ ë¦¬í„´
@st.cache_resource
def load_vectorstore():
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=st.secrets["OPENAI_API_KEY"]
    )
    return FAISS.load_local("faiss_index/db", embeddings)

def search_products(ingredient_query, avoid=None, top_k: int = 5):
    """
    ingredient_query : ì¶”ì²œ ì„±ë¶„(ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì‰¼í‘œ ë¬¸ìì—´)
    avoid           : ì œì™¸í•  ì„±ë¶„ ë¦¬ìŠ¤íŠ¸(ì˜µì…˜)
    top_k           : ìµœì¢… ë°˜í™˜ ê°œìˆ˜
    """
    
    db = load_vectorstore()

    # â”€â”€ [A] ì…ë ¥ ì „ì²˜ë¦¬ & ë¡œê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if isinstance(ingredient_query, list):
        ingredient_query = ", ".join(ingredient_query)
    # avoidê°€ ë¬¸ìì—´ì´ë©´ ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬
    if isinstance(avoid, str):
        avoid = [a.strip() for a in avoid.split(",") if a.strip()]

    print(f"\n[DEBUG] ingredient_query = {ingredient_query}")
    print(f"[DEBUG] avoid            = {avoid}")

    # â”€â”€ [B] í›„ë³´êµ° ê²€ìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    candidate_docs = db.similarity_search(ingredient_query, k=max(1, top_k * 4))
    print(f"[DEBUG] candidate_docs   = {len(candidate_docs)}ê°œ")

    # â”€â”€ [C] â€˜avoidâ€™ ì„±ë¶„ í•„í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if avoid:
        avoid_lower = [a.lower() for a in avoid]
        filtered_docs = []

        for doc in candidate_docs:
            text_lower = doc.page_content.lower()
            # í˜„ì¬ ë¬¸ì„œì—ì„œ ë°œê²¬ëœ í”¼í•´ì•¼ í•  ì„±ë¶„ ëª©ë¡
            matched = [a for a in avoid_lower if a in text_lower]

            if matched:
                # ì–´ë–¤ ì„±ë¶„ ë•Œë¬¸ì— ì œì™¸ëëŠ”ì§€ í‘œì‹œ
                product_name = doc.metadata.get("product", "ì´ë¦„ì—†ìŒ")
                print(f"  â†ªï¸ [FILTER] {product_name}  (ì œì™¸ ì´ìœ : {', '.join(matched)})")
                continue

            filtered_docs.append(doc)
            if len(filtered_docs) >= top_k:
                break

        print(f"[DEBUG] filtered_docs    = {len(filtered_docs)}ê°œ ìµœì¢… ë°˜í™˜\n")
        return filtered_docs

    # â”€â”€ [D] í•„í„°ë§ ë¶ˆí•„ìš” ì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[DEBUG] filtered_docs    = {top_k}ê°œ ìµœì¢… ë°˜í™˜ (í•„í„° ì—†ìŒ)\n")
    return candidate_docs[:top_k]
