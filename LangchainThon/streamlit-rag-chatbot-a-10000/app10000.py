# app.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â… . Import & í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import json
import streamlit as st
from api10000 import search_products
from typing import List

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain.retrievers import EnsembleRetriever

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
st.set_page_config(page_title="ì˜ì–‘ì œ Check ì±—ë´‡", page_icon="ğŸ’Š")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¡. PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
# PDFë¥¼ ì¹´í…Œê³ ë¦¬ í´ë” ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ê³  LangChain Documentë¡œ ë³€í™˜
def load_and_split_pdfs_by_category(base_path: str) -> dict:
    base_path = os.path.join(os.path.dirname(__file__), base_path)
    
    all_docs_by_category = {}
    for category in os.listdir(base_path):
        category_path = os.path.join(base_path, category)
        if os.path.isdir(category_path):
            docs = []
            for f in os.listdir(category_path):
                if f.endswith(".pdf"):
                    docs.extend(PyPDFLoader(os.path.join(category_path, f)).load())
            all_docs_by_category[category] = docs
    return all_docs_by_category


@st.cache_resource
# ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í•  (1000ì, ì¤‘ë³µ 150ì)
# OpenAI Embeddingìœ¼ë¡œ ë²¡í„° ìƒì„±
# FAISS ì¸ë±ìŠ¤ë¥¼ ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ê±°ë‚˜ ë¡œë“œ
# -> ìš´ì˜ í™˜ê²½ ë°°í¬ ì‹œì—ëŠ” .pkl ì œê±° ë° faiss + json ë°©ì‹ ê¶Œì¥
def create_vectorstore_per_category(_docs_by_category: dict) -> dict:
    vectorstores = {}
    for category, docs in _docs_by_category.items():
        persist_dir = f"./faiss_index/{category}"
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

        if os.path.exists(os.path.join(persist_dir, "index.faiss")):
            # ì´ë¯¸ ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ë¡œë“œ
            vectorstores[category] = FAISS.load_local(
                persist_dir,
                embeddings,
                allow_dangerous_deserialization=True  # ë³´ì•ˆì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ Pickle ì‚¬ìš©ì„ ê¸ˆì§€
            )
        else:
            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ì €ì¥
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
            split_docs = text_splitter.split_documents(docs)
            faiss_index = FAISS.from_documents(split_docs, embeddings)
            faiss_index.save_local(persist_dir)
            vectorstores[category] = faiss_index

    return vectorstores

# ì‚¬ìš©ì ì¿¼ë¦¬ì— í¬í•¨ëœ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì„ íƒ
# ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ëŒ€ì‘ (Case A~D ì „ìš©)
def categorize_user_query(query: str) -> List[str]:
    """ì‹œì—°ìš©: ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¼€ì´ìŠ¤ A~Dì— ë§ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    mapping = {
        "category_A": ["ìˆ ", "ë‹´ë°°", "ì»¤í”¼", "ì¢…í•©ë¹„íƒ€ë¯¼", "ëˆˆ ê±´ê°•", "ê°„ íšŒë³µ"],
        "category_B": ["ìš´ë™", "ì‹ìŠµê´€", "ì¢…í•©ë¹„íƒ€ë¯¼", "ëˆˆ ê±´ê°•", "ê·¼ìœ¡ íšŒë³µ"],
        "category_C": ["ë‹¹ë‡¨ë³‘", "ê³ í˜ˆì••", "ì‹¬í˜ˆê´€", "í˜ˆì••"],
        "category_D": ["ê´€ì ˆì—¼", "ì¹˜ë§¤", "ì•„ë¦¬ì…‰íŠ¸", "ë‡Œ ê±´ê°•"],
    }

    query_lower = query.lower()
    selected = []

    for category, keywords in mapping.items():
        if all(keyword.lower() in query_lower for keyword in keywords):
            selected.append(category)

    return list(set(selected))


# ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ retrieverë“¤ì„ EnsembleRetrieverë¡œ í†µí•©
# -> create_history_aware_retrieverë¥¼ í†µí•´ ëŒ€í™” ê¸°ë°˜ ì§ˆë¬¸ ë¦¬í¬ë§·
# -> QA Prompt êµ¬ì„± í›„ create_retrieval_chainìœ¼ë¡œ RAG ì™„ì„±
def build_rag_chain_from_categories(categories: List[str], vectorstores: dict):
    # combined_docs = []
    for cat in categories:
        retriever = vectorstores[cat].as_retriever()

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)
    retrievers = [
    vectorstores[cat].as_retriever(search_kwargs={"k": 8})
    for cat in categories
]
    retriever = EnsembleRetriever(retrievers=retrievers)

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question relating to dietary supplements, diseases, or schedules, reformulate a standalone question. Do NOT answer the question."),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",  """ë‹¹ì‹ ì€ ê°œì¸ ë§ì¶¤ ì˜ì–‘ì œ ì½”ì¹˜ì…ë‹ˆë‹¤. ì•„ë˜ ì‚¬ìš©ì ì •ë³´ë¥¼ ë¶„ì„í•´ ì‚¬ìš©ìì—ê²Œ í•„ìš”í•œ ì˜ì–‘ì œ ì„±ë¶„ê³¼ í”¼í•´ì•¼ í•  ì˜ì–‘ì œ ì„±ë¶„ì„ ì•Œë ¤ì£¼ì„¸ìš”.

[ì„±ë¶„ ì„ ì • ì›ì¹™]
ì¶”ì²œ ì˜ì–‘ì œ ì„±ë¶„ ëª©ë¡, í”¼í•´ì•¼ í•  ì˜ì–‘ì œ ì„±ë¶„ ëª©ë¡ì€ ë¬¸í—Œì— ê·¼ê±°í•˜ì—¬ ì‘ì„±í•´ì•¼ í•˜ë©°, ëª…ì‹œì  ì¶œì²˜ë¥¼ ì œì‹œí•˜ì„¸ìš”.
ë¬¸í—Œ ë‚´ ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°, â€œì •ë³´ ë¶€ì¡±â€ ë˜ëŠ” â€œê·¼ê±° ì—†ìŒâ€ì´ë¼ê³  ëª…ì‹œí•´ì£¼ì„¸ìš”.
ì¶”ì²œ ì„±ë¶„ ëª©ë¡ì— â€œì˜ì–‘ì œì˜ ì„±ë¶„â€ì— ì§‘ì¤‘í•˜ì—¬ ë¶„ì„í•˜ê³ , ì œí’ˆëª…ì´ë‚˜ ìƒí™œìŠµê´€ ìš”ì†Œ(ì˜ˆ: ì»¤í”¼, ì•Œì½”ì˜¬, ë…¹ì°¨ ë“±)ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
í”¼í•´ì•¼ í•  ì„±ë¶„ë„ ë°˜ë“œì‹œ â€œì˜ì–‘ì œì˜ ì„±ë¶„â€ì— ì§‘ì¤‘í•˜ì—¬ ë¶„ì„í•˜ê³ , ì œí’ˆëª…ì´ë‚˜ ìƒí™œìŠµê´€ ìš”ì†Œ(ì˜ˆ: ì»¤í”¼, ì•Œì½”ì˜¬, ë…¹ì°¨, ê³ êµ¬ë§ˆ ë“±)ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
ì„±ë¶„ëª…ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì„±ë¶„ëª…ì—ëŠ” ê³µë°±ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
ì£¼ì˜ì‚¬í•­ì€ ì‚¬ìš©ìì—ê²Œ ê±´ê°• ê´€ë ¨ ì£¼ì˜í•  ì‚¬í•­ì„ í•œ, ë‘ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”

[ì¶œë ¥ í˜•ì‹(ìˆœìˆ˜ JSON)]
{{
  "recommended": ["ì„±ë¶„1", "ì„±ë¶„2", ...],
  "avoid":       ["ì„±ë¶„A", "ì„±ë¶„B", ...],
  "cautions":    ["ì£¼ì˜ì‚¬í•­ ...", ...],
  "sources":     ["ì¶œì²˜1", "ì¶œì²˜2", ...]
}}

__ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ë¶€ê°€ ì„¤ëª… ì—†ì´ ìœ„ JSON í•œ ë©ì–´ë¦¬ë§Œ ë°˜í™˜__í•˜ì„¸ìš”.

{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¢. LangChain ì»´í¬ë„ŒíŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @st.cache_resource
# : queryì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê³ ë¥´ê¸° ë•Œë¬¸ì— ìºì‹œí•˜ë©´ ì´ì „ ì§ˆì˜ì™€ ê²°ê³¼ê°€ ì„ì—¬ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ì¤„ ìˆ˜ ì„ìŒ
# RAG ì²´ì¸ ì´ˆê¸°í™”
# : ë¬¸ì„œ ë¡œë”© + ë²¡í„° ìƒì„±/ë¡œë“œ + ì¹´í…Œê³ ë¦¬ ì„ íƒ + ì²´ì¸ ìƒì„±
def initialize_components( query: str):
    """ì˜ì–‘ì œ ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ ì´ˆê¸°í™” (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)"""
    docs_by_cat = load_and_split_pdfs_by_category("./data/supplement_knowledge")
    vectorstores = create_vectorstore_per_category(docs_by_cat)
    categories = categorize_user_query(query)
    return build_rag_chain_from_categories(categories, vectorstores)


def build_user_query(age: int, gender: str, disease: str, taking: str, prefers: str) -> str:
    return (
        f"ë‹¤ìŒì€ ê°œì¸ ë§ì¶¤ ì˜ì–‘ì œë¥¼ ì¶”ì²œë°›ê³ ì í•˜ëŠ” ì‚¬ìš©ì ì •ë³´ì…ë‹ˆë‹¤.\n\n"
        f"[ì‚¬ìš©ì ì •ë³´]\n"
        f"- ë‚˜ì´/ì„±ë³„: {age}ì„¸ {gender}\n"
        f"- ì§ˆí™˜/ì¦ìƒ/íŠ¹ì´ì‚¬í•­: {disease}\n"
        f"- í˜„ì¬ ë³µìš© ì¤‘ì¸ ì•½ì œ ë˜ëŠ” ì˜ì–‘ì œ: {taking}\n"
        f"- ì›í•˜ëŠ” ì˜ì–‘ì œ ê¸°ëŠ¥ ë˜ëŠ” íš¨ê³¼: {prefers}\n\n"
        "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ì„±ë¶„ì„ ì¶”ì²œí•˜ê±°ë‚˜ í”¼í•´ì•¼ í• ì§€ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…£. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ’Š ì˜ì–‘ì œ Check!")

chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ì…ë ¥ í¼
with st.form("user_profile_form"):
    st.markdown("#### ğŸ‘¤ ì‚¬ìš©ì ê±´ê°• ì •ë³´ ì…ë ¥")

    # ì²« ì¤„: ì™¼ìª½ = ë‚˜ì´Â·ì„±ë³„, ì˜¤ë¥¸ìª½ = ì§ˆí™˜Â·ì¦ìƒ
    top_left, top_right = st.columns(2)
    with top_left:
        age = st.number_input("ë‚˜ì´", min_value=0, max_value=120, step=1, placeholder="ì˜ˆ: 35")
        gender = st.selectbox("ì„±ë³„", ["ì—¬ì„±", "ë‚¨ì„±", "ê¸°íƒ€"])
    with top_right:
        disease = st.text_area("â‘  ì§ˆí™˜Â·ì¦ìƒÂ·íŠ¹ì´ì‚¬í•­", placeholder="ì˜ˆ) ë§Œì„± ìœ„ì—¼, ê³ ì§€í˜ˆì¦")

    # ë‘ ë²ˆì§¸ ì¤„: ì™¼ìª½ = ë³µìš© ì¤‘, ì˜¤ë¥¸ìª½ = ì›í•˜ëŠ” íŠ¹ì§•
    bottom_left, bottom_right = st.columns(2)
    with bottom_left:
        taking = st.text_area("â‘¡ í˜„ì¬ ë³µìš© ì¤‘ì¸ ì˜ì–‘ì œÂ·ì•½í’ˆ", placeholder="ì˜ˆ) ì¢…í•©ë¹„íƒ€ë¯¼")
    with bottom_right:
        prefers = st.text_area("â‘¢ ì›í•˜ëŠ” ì˜ì–‘ì œ íŠ¹ì§•", placeholder="ì˜ˆ) í”¼ë¡œ íšŒë³µ, ê°„ ë³´í˜¸, í•˜ë£¨ í•œ ì•Œ")

    submitted = st.form_submit_button("ì¶”ì²œë°›ê¸°")

# ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
for m in chat_history.messages:
    st.chat_message(m.type).write(m.content)

# ì œì¶œ ì´ë²¤íŠ¸
if submitted:
    if not all([age, gender, disease, taking, prefers]):
        st.warning("í•­ëª©ì„ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    user_query = build_user_query(age, gender, disease, taking, prefers)
    rag_chain = initialize_components(user_query)
    
    conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
    )

    st.chat_message("human").write(user_query)

    with st.chat_message("ai"):
        with st.spinner("ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": user_query}, config)
            # answer = response["answer"]
            # st.write(answer)
            answer_raw = response["answer"]

            # â”€â”€ JSON íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                answer_json = json.loads(answer_raw)
            except json.JSONDecodeError:
                st.error("âš ï¸ LLM ë‹µë³€ì„ JSONìœ¼ë¡œ í•´ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.stop()
            
            # JSON íŒŒì‹±
            recommended = answer_json.get("recommended", [])
            avoid = answer_json.get("avoid", [])
            cautions = answer_json.get("cautions", {})
            sources = answer_json.get("sources", [])

            # êµ¬ì¡°í™” ì¶œë ¥
            st.subheader("ğŸ“ì‚¬ìš©ì ê±´ê°• ê¸°ë°˜ ë¶„ì„")

            with st.expander("âœ… ì¶”ì²œ ì„±ë¶„"):
                if recommended:
                    st.markdown("\n".join([f"- {item}" for item in recommended]))
                else:
                    st.write("ì¶”ì²œ ì„±ë¶„ ì—†ìŒ.")

            with st.expander("ğŸš« í”¼í•´ì•¼ í•  ì„±ë¶„"):
                if avoid:
                    st.markdown("\n".join([f"- {item}" for item in avoid]))
                else:
                    st.write("í”¼í•´ì•¼ í•  ì„±ë¶„ ì—†ìŒ.")

            with st.expander("âš ï¸ ì£¼ì˜ì‚¬í•­"):
                if cautions:
                    if isinstance(cautions, list):
                            for line in cautions:
                                st.markdown(f"- {line}")
                    elif isinstance(cautions, str):
                            st.markdown(f"- {cautions}")
                    else:
                            st.write("ì£¼ì˜ì‚¬í•­ í˜•ì‹ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.write("íŠ¹ë³„í•œ ì£¼ì˜ì‚¬í•­ ì—†ìŒ.")

            with st.expander("ğŸ“š ì¶œì²˜"):
                if sources:
                    for s in sources:
                        st.markdown(f"- {s}")
                else:
                    st.write("ì¶œì²˜ ì—†ìŒ.")

            # ì¶”ì²œ ì„±ë¶„ì´ ìˆìœ¼ë©´ â†’ api.py ë¡œ ì „ë‹¬ â†’ ê´€ë ¨ ì œí’ˆ 5ê°œ ê²€ìƒ‰
            if recommended:
                product_docs = search_products(recommended, avoid=avoid, top_k=5)

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                #   Streamlit í™”ë©´ì— ê²°ê³¼ í‘œì‹œ
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if product_docs:
                    st.subheader("ğŸ’¡ ë§ì¶¤ ì˜ì–‘ì œ ì¶”ì²œ")

                    for idx, doc in enumerate(product_docs, 1):
                        name = doc.metadata.get("product", f"ì œí’ˆ {idx}")
                        content = doc.page_content.strip()

                        with st.expander(f"{idx}. {name}"):
                            # ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°œì„œ í•œ ì¤„ì”© ì¶œë ¥
                            for line in content.split("\n"):
                                line = line.strip()
                                if line:
                                    st.write(line)
                else:
                    st.info("ì¶”ì²œ ì„±ë¶„ê³¼ ì—°ê´€ëœ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë©´ì±… ê³ ì§€
st.caption("âš ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” ì¼ë°˜ì ì¸ ê±´ê°• ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, "
           "ê°œì¸ ì²˜ë°©ì´ ì•„ë‹™ë‹ˆë‹¤. ë³µìš© ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.")

